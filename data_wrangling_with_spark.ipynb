{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark import SparkConf\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import desc\n",
    "from pyspark.sql.functions import col, desc, udf, count, sum, avg, array\n",
    "\n",
    "from pyspark.sql.functions import sum as Fsum\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "import hashlib\n",
    "from typing import List\n",
    "import findspark\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [1,2]# a = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-23\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from pytz import timezone\n",
    "\n",
    "# Define the Pacific Time Zone\n",
    "pacific = timezone('America/Los_Angeles')\n",
    "\n",
    "# Get the current time in Pacific Time\n",
    "pacific_time = datetime.now(pacific)\n",
    "\n",
    "# Format the date as \"yyyy-mm-dd\"\n",
    "pacific_date_str = pacific_time.strftime('%Y-%m-%d')\n",
    "\n",
    "print(pacific_date_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data\n",
    "\n",
    "### From Local File or From S3 AWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/23 11:48:32 WARN Utils: Your hostname, noah-Alienware-U18 resolves to a loopback address: 127.0.1.1; using 10.0.0.109 instead (on interface wlp4s0)\n",
      "24/11/23 11:48:32 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/11/23 11:48:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "findspark.init()\n",
    "\n",
    "\n",
    "def print_dataframe_info(df, num_sample_rows=3):\n",
    "    \"\"\"\n",
    "    Prints detailed information about a PySpark DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    df (DataFrame): The PySpark DataFrame to analyze.\n",
    "    num_sample_rows (int): Number of sample rows to display.\n",
    "    \"\"\"\n",
    "    # Print the schema of the DataFrame\n",
    "    print(\"Schema:\")\n",
    "    df.printSchema()\n",
    "    \n",
    "    # Show a sample of rows from the DataFrame\n",
    "    print(f\"\\nSample Rows (first {num_sample_rows} rows):\")\n",
    "    df.show(num_sample_rows)\n",
    "    \n",
    "    # Show descriptive statistics for numerical columns\n",
    "    print(\"\\nDescriptive Statistics:\")\n",
    "    df.describe().show()\n",
    "    \n",
    "    # Print all column names\n",
    "    column_names = df.columns\n",
    "    print(\"\\nColumn Names:\", column_names)\n",
    "    \n",
    "    # Get the number of rows\n",
    "    num_rows = df.count()\n",
    "    print(\"Number of Rows:\", num_rows)\n",
    "    \n",
    "    # Get the number of columns\n",
    "    num_columns = len(column_names)\n",
    "    print(\"Number of Columns:\", num_columns)\n",
    "\n",
    "\n",
    "LOAD_TYPE = \"LOCAL\"\n",
    "file_path = \" \"\n",
    "SECRET = \"xxNWgbGcHrXD2lRFKDtcZjwdwHwDBqZuGpifjpJA\"\n",
    "KEY = \"AKIAWFSXZJYBJ7TYXUPQ\"\n",
    "\n",
    "\n",
    "#Create a Spark Context\n",
    "local_path = \"data/song_data/*/*/*/*.json\"\n",
    "s3_path = \"s3://udacity-dend/song_data/*/*/*/*.json\"\n",
    "\n",
    "\n",
    "sc = SparkContext()\n",
    "dataset_name = \"SongDataSet\"\n",
    "if LOAD_TYPE == \"LOCAL\":\n",
    "        spark = SparkSession\\\n",
    "                        .builder \\\n",
    "                        .appName(dataset_name)\\\n",
    "                        .getOrCreate()\n",
    "        file_path = local_path\n",
    "\n",
    "else: \n",
    "        conf = SparkConf()\n",
    "        conf.set('spark.jars.packages', 'org.apache.hadoop:hadoop-aws:3.3.1')\n",
    "        conf.set('spark.hadoop.fs.s3a.aws.credentials.provider',  'org.apache.hadoop.fs.s3a.AnonymousAWSCredentialsProvider')#'org.apache.hadoop.fs.s3a.TemporaryAWSCredentialsProvider')\n",
    "        conf.set('spark.hadoop.fs.s3a.access.key', KEY)\n",
    "        conf.set('spark.hadoop.fs.s3a.secret.key', SECRET)\n",
    "        # conf.set('spark.hadoop.fs.s3a.session.token', <token>)\n",
    "        spark = SparkSession.builder \\\n",
    "                        .appName(dataset_name) \\\n",
    "                        .config(conf = conf) \\\n",
    "                        .getOrCreate()\n",
    "        # hadoop_conf.set(\"fs.s3a.endpoint\", \"s3.amazonaws.com\")\n",
    "        file_path = s3_path\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+---------------+--------------------+----------------+--------------------+---------+---------+------------------+--------------------+----+\n",
      "|         artist_id|artist_latitude|     artist_location|artist_longitude|         artist_name| duration|num_songs|           song_id|               title|year|\n",
      "+------------------+---------------+--------------------+----------------+--------------------+---------+---------+------------------+--------------------+----+\n",
      "|AR4T2IF1187B9ADBB7|       63.96027|<a href=\"http://b...|        10.22442|          Billy Idol|233.22077|        1|SOVIYJY12AF72A4B00|The Dead Next Doo...|1983|\n",
      "|AR4T2IF1187B9ADBB7|       63.96027|<a href=\"http://b...|        10.22442|          Billy Idol|287.92118|        1|SOVYXYL12AF72A3373|Rebel Yell (1999 ...|1983|\n",
      "|ARQ846I1187B9A7083|           NULL|                    |            NULL|Yvonne S. Moriart...|196.04853|        1|SOEPTVC12A67ADD0DA|To Zucchabar [\"Gl...|   0|\n",
      "+------------------+---------------+--------------------+----------------+--------------------+---------+---------+------------------+--------------------+----+\n",
      "only showing top 3 rows\n",
      "\n",
      "root\n",
      " |-- artist_id: string (nullable = true)\n",
      " |-- artist_latitude: double (nullable = true)\n",
      " |-- artist_location: string (nullable = true)\n",
      " |-- artist_longitude: double (nullable = true)\n",
      " |-- artist_name: string (nullable = true)\n",
      " |-- duration: double (nullable = true)\n",
      " |-- num_songs: long (nullable = true)\n",
      " |-- song_id: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- year: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create an RDD from the JSON data\n",
    "# json_rdd = sc.parallelize(s3_path)\n",
    "# # Convert the RDD to a DataFrame\n",
    "# songs_df = spark.read.json(json_rdd)\n",
    "# # Coalesce dataframes to a single partition: this helps to avoid having empty files due to parallelization\n",
    "# songs_df = songs_df.coalesce(1)\n",
    "songs_df = spark.read.json(file_path)\n",
    "\n",
    "# Show the DataFrame\n",
    "songs_df.printSchema()\n",
    "songs_df.show(3)\n",
    "songs_df.describe().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- artist: string (nullable = true)\n",
      " |-- auth: string (nullable = true)\n",
      " |-- firstName: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- itemInSession: long (nullable = true)\n",
      " |-- lastName: string (nullable = true)\n",
      " |-- length: double (nullable = true)\n",
      " |-- level: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- method: string (nullable = true)\n",
      " |-- page: string (nullable = true)\n",
      " |-- registration: double (nullable = true)\n",
      " |-- sessionId: long (nullable = true)\n",
      " |-- song: string (nullable = true)\n",
      " |-- status: long (nullable = true)\n",
      " |-- ts: long (nullable = true)\n",
      " |-- userAgent: string (nullable = true)\n",
      " |-- userId: string (nullable = true)\n",
      "\n",
      "+-----------+---------+---------+------+-------------+--------+---------+-----+--------------------+------+--------+-----------------+---------+---------------+------+-------------+--------------------+------+\n",
      "|     artist|     auth|firstName|gender|itemInSession|lastName|   length|level|            location|method|    page|     registration|sessionId|           song|status|           ts|           userAgent|userId|\n",
      "+-----------+---------+---------+------+-------------+--------+---------+-----+--------------------+------+--------+-----------------+---------+---------------+------+-------------+--------------------+------+\n",
      "|   Harmonia|Logged In|     Ryan|     M|            0|   Smith|655.77751| free|San Jose-Sunnyval...|   PUT|NextSong|1.541016707796E12|      583|  Sehr kosmisch|   200|1542241826796|\"Mozilla/5.0 (X11...|    26|\n",
      "|The Prodigy|Logged In|     Ryan|     M|            1|   Smith|260.07465| free|San Jose-Sunnyval...|   PUT|NextSong|1.541016707796E12|      583|The Big Gundown|   200|1542242481796|\"Mozilla/5.0 (X11...|    26|\n",
      "|      Train|Logged In|     Ryan|     M|            2|   Smith|205.45261| free|San Jose-Sunnyval...|   PUT|NextSong|1.541016707796E12|      583|       Marry Me|   200|1542242741796|\"Mozilla/5.0 (X11...|    26|\n",
      "+-----------+---------+---------+------+-------------+--------+---------+-----+--------------------+------+--------+-----------------+---------+---------------+------+-------------+--------------------+------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+-------+------------------+----------+---------+------+------------------+--------+------------------+-----+--------------------+------+-------+--------------------+------------------+--------------------+------------------+--------------------+--------------------+------------------+\n",
      "|summary|            artist|      auth|firstName|gender|     itemInSession|lastName|            length|level|            location|method|   page|        registration|         sessionId|                song|            status|                  ts|           userAgent|            userId|\n",
      "+-------+------------------+----------+---------+------+------------------+--------+------------------+-----+--------------------+------+-------+--------------------+------------------+--------------------+------------------+--------------------+--------------------+------------------+\n",
      "|  count|              6820|      8056|     7770|  7770|              8056|    7770|              6820| 8056|                7770|  8056|   8056|                7770|              8056|                6820|              8056|                8056|                7770|              8056|\n",
      "|   mean|             266.5|      NULL|     NULL|  NULL| 21.19885799404171|    NULL|247.03222091348965| NULL|                NULL|  NULL|   NULL|1.540777905900890...| 598.1675769612712|  1388.3636363636363|  202.897591857001|1.542486261744982...|                NULL| 54.46396396396396|\n",
      "| stddev|109.00229355385143|      NULL|     NULL|  NULL|23.440698529423255|    NULL|102.97592081740876| NULL|                NULL|  NULL|   NULL|2.6515717661170202E8|285.31309422188014|  2347.5150807919117|17.994255629646254|  7.00316630205993E8|                NULL|28.168503533430705|\n",
      "|    min|               !!!| Logged In|   Adelyn|     F|                 0|Arellano|          15.85587| free|Atlanta-Sandy Spr...|   GET|  About|   1.539908999796E12|                 3| I Will Not Reap ...|               200|       1541105830796|\"Mozilla/5.0 (Mac...|                  |\n",
      "|    max|   ÃÂtienne Daho|Logged Out|  Zachary|     M|               127|   Young|        2594.87302| paid|       Yuba City, CA|   PUT|Upgrade|   1.541098488796E12|              1114|ÃÂ Aqui Que Se ...|               404|       1543607664796|Mozilla/5.0 (comp...|                99|\n",
      "+-------+------------------+----------+---------+------+------------------+--------+------------------+-----+--------------------+------+-------+--------------------+------------------+--------------------+------------------+--------------------+--------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "event_dir = \"data/2018/*/*.json\"\n",
    "events_df = spark.read.json(event_dir)\n",
    "\n",
    "# Show the DataFrame\n",
    "events_df.printSchema()\n",
    "events_df.show(3)\n",
    "events_df.describe().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CREATE TABLE\n",
    "### CREATE TEMPORALY SQL VIEW IN SPARK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_df.createOrReplaceTempView(\"events_table\")\n",
    "songs_df.createOrReplaceTempView(\"songs_table\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimension Tables\n",
    "\n",
    "#### 1) Users Dimension Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+---------+------+-----+\n",
      "|user_id|first_name|last_name|gender|level|\n",
      "+-------+----------+---------+------+-----+\n",
      "|     84|   Shakira|     Hunt|     F| free|\n",
      "|     52|  Theodore|    Smith|     M| free|\n",
      "|     80|     Tegan|   Levine|     F| paid|\n",
      "|     15|      Lily|     Koch|     F| paid|\n",
      "|     37|    Jordan|    Hicks|     F| free|\n",
      "|     43|    Jahiem|    Miles|     M| free|\n",
      "|     44|    Aleena|    Kirby|     F| paid|\n",
      "|     50|       Ava| Robinson|     F| free|\n",
      "|     63|      Ayla|  Johnson|     F| free|\n",
      "|     30|     Avery|  Watkins|     F| paid|\n",
      "|     92|     Ryann|    Smith|     F| free|\n",
      "|     40|    Tucker| Garrison|     M| free|\n",
      "|     16|     Rylan|   George|     M| paid|\n",
      "|     88|  Mohammad|Rodriguez|     M| paid|\n",
      "|     54|     Kaleb|     Cook|     M| free|\n",
      "|     96|    Cierra|   Finley|     F| free|\n",
      "|     26|      Ryan|    Smith|     M| free|\n",
      "|     36|   Matthew|    Jones|     M| paid|\n",
      "|     12|    Austin|  Rosales|     M| free|\n",
      "|     74|    Braden|   Parker|     M| free|\n",
      "+-------+----------+---------+------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "users_dim_df = events_df.select('userId','firstName','lastName',\n",
    "                             'gender','level')\\\n",
    "                    .dropDuplicates()\n",
    "rename_dict ={\"userId\" :\"user_id\",\n",
    "              \"firstName\" : \"first_name\",\n",
    "              \"lastName\" : \"last_name\",\n",
    "              \"gender\" : \"gender\",\n",
    "              \"level\" : \"level\"}\n",
    "# Loop through the dictionary to rename columns\n",
    "for old_name, new_name in rename_dict.items():\n",
    "    users_dim_df = users_dim_df.withColumnRenamed(old_name, new_name)\n",
    "\n",
    "# Show the DataFrame with renamed columns\n",
    "users_dim_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+---------+------+-----+\n",
      "|user_id|first_name|last_name|gender|level|\n",
      "+-------+----------+---------+------+-----+\n",
      "|     26|      Ryan|    Smith|     M| free|\n",
      "|     26|      Ryan|    Smith|     M| free|\n",
      "|     26|      Ryan|    Smith|     M| free|\n",
      "|      9|     Wyatt|    Scott|     M| free|\n",
      "|     12|    Austin|  Rosales|     M| free|\n",
      "|     61|    Samuel| Gonzalez|     M| free|\n",
      "|     61|    Samuel| Gonzalez|     M| free|\n",
      "|       |      NULL|     NULL|  NULL| paid|\n",
      "|     80|     Tegan|   Levine|     F| paid|\n",
      "|     80|     Tegan|   Levine|     F| paid|\n",
      "|     80|     Tegan|   Levine|     F| paid|\n",
      "|     80|     Tegan|   Levine|     F| paid|\n",
      "|     80|     Tegan|   Levine|     F| paid|\n",
      "|     80|     Tegan|   Levine|     F| paid|\n",
      "|     80|     Tegan|   Levine|     F| paid|\n",
      "|     80|     Tegan|   Levine|     F| paid|\n",
      "|     80|     Tegan|   Levine|     F| paid|\n",
      "|     15|      Lily|     Koch|     F| paid|\n",
      "|     80|     Tegan|   Levine|     F| paid|\n",
      "|     15|      Lily|     Koch|     F| paid|\n",
      "+-------+----------+---------+------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "users_dim_sql_df= spark.sql('''\n",
    "          SELECT userID AS user_id,\n",
    "                 firstName AS first_name,\n",
    "                 lastName AS last_name,\n",
    "                 gender,\n",
    "                 level\n",
    "          FROM events_table\n",
    "          '''\n",
    "          )\n",
    "        # WHERE userID IS NOT NULL\n",
    "        #   DROP DUPLICATES\n",
    "users_dim_sql_df.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Users Dimension Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 20:===================================================> (452 + 14) / 466]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------+------------------+----+----------+\n",
      "|           song_id|               title|         artist_id|year|  duration|\n",
      "+------------------+--------------------+------------------+----+----------+\n",
      "|SOVYXYL12AF72A3373|Rebel Yell (1999 ...|AR4T2IF1187B9ADBB7|1983| 287.92118|\n",
      "|SONVWOX12A8C137FF5|Le Roi de Lahore_...|ARIDEBJ1187B9A3754|   0| 250.48771|\n",
      "|SOROAMT12A8C13C6D0|Me gustan mas los...|ARWUDTF1187B9AA096|2008| 101.85098|\n",
      "|SOVBRVG12A8C13DDCE|Millions starved ...|ARYOIZG1187FB41E30|2006| 594.75546|\n",
      "|SOIGELY12A6D4F65C6|Aida (1986 Digita...|ARXLQYU11F4C83E050|   0| 345.25995|\n",
      "|SOGXGDI12AB017F83B|You Can't Stop Th...|ARD63361187FB52CA6|   0| 324.67546|\n",
      "|SOUQKQA12A6D4F951E|Perfume: The Stor...|ARZEHBT124549A4599|   0| 331.20608|\n",
      "|SOGZKUT12AB0186276|   Tierra Sin Nombre|ARJNVPB12454A3E0B8|   0|  180.1922|\n",
      "|SOBTCUI12A8AE48B70|Faust: Ballet Mus...|ARSUVLW12454A4C8B8|   0|  94.56281|\n",
      "|SOSHXLZ12AB0185894|Throw Them 3's (B...|ARAQMES1187FB4D46A|   0|  260.8322|\n",
      "|SOKEJTW12A6D4F8D66|Don't Tell (Expli...|ARZ5H0P1187B98A1DD|1999| 287.08526|\n",
      "|SOVIYJY12AF72A4B00|The Dead Next Doo...|AR4T2IF1187B9ADBB7|1983| 233.22077|\n",
      "|SOMDBLQ12A6D4F8B72|Quartet for Piano...|AREVPHS1187FB3B6EB|   0| 496.53506|\n",
      "|SOLQYSZ12AB0181F97|    Mony Mony (Live)|AR4T2IF1187B9ADBB7|1987| 247.53587|\n",
      "|SODUMDU12AC468A22B|We're Skrewed (Ot...|ARHOSMU1242078130D|   0|  249.5473|\n",
      "|SOTCIHX12A8C13DDD2|Finally_ as that ...|ARYOIZG1187FB41E30|2006| 483.34322|\n",
      "|SOUFBFK12A8C13D668|String Quartets O...|ARAILTA11F4C840A06|   0| 348.60363|\n",
      "|SOHPSTY12A6D4F719C|Ms. New Booty (Ed...|ARJYD111187FB3AC15|   0| 252.52526|\n",
      "|SOAZUNV12A8C13923B|A Faust Symphony ...|ARSUVLW12454A4C8B8|   0|1237.86404|\n",
      "|SOUHAOJ12AB0187C31|Te Deum Laudamus_...|ARN8NCB1187FB49652|   0|  70.03383|\n",
      "+------------------+--------------------+------------------+----+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "songs_dim_df = songs_df.select('song_id','title','artist_id','year','duration')\\\n",
    "                    .dropDuplicates()\n",
    "songs_dim_df.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-----------------+------------------+------------------+------------------+\n",
      "|summary|           song_id|            title|         artist_id|              year|          duration|\n",
      "+-------+------------------+-----------------+------------------+------------------+------------------+\n",
      "|  count|             14896|            14896|             14896|             14896|             14896|\n",
      "|   mean|              NULL|581.6631578947369|              NULL|1360.5122851772287|246.77939752953853|\n",
      "| stddev|              NULL|876.8867785844078|              NULL| 932.6891909627184|110.00572654979545|\n",
      "|    min|SOAAAQN12AB01856D3|  Dry Techterlech|AR00B1I1187FB433EB|                 0|           6.37342|\n",
      "|    max|SOZZZON12A8C139ED5|             Üres|ARZZXT51187FB4627E|              2010|         2709.2371|\n",
      "+-------+------------------+-----------------+------------------+------------------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 29:==================================================>  (445 + 16) / 466]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------+------------------+----+---------+\n",
      "|           song_id|               title|         artist_id|year| duration|\n",
      "+------------------+--------------------+------------------+----+---------+\n",
      "|SOLLALT12A8C1399F3|Piano Concerto No...|ARWMEJW11F4C83C123|   0|319.37261|\n",
      "|SOLQYSZ12AB0181F97|    Mony Mony (Live)|AR4T2IF1187B9ADBB7|1987|247.53587|\n",
      "|SOLXXRJ12A6D4F7260|Firepower (The Ta...|AR0CANF1187B9AF35F|2006|311.06567|\n",
      "+------------------+--------------------+------------------+----+---------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "songs_dim_sql_df = spark.sql(\"\"\"\n",
    "                             SELECT DISTINCT song_id,\n",
    "                                    title,\n",
    "                                    artist_id,\n",
    "                                    year,\n",
    "                                    duration\n",
    "                             FROM songs_table;\n",
    "                             \"\"\")\n",
    "songs_dim_sql_df.describe().show()\n",
    "songs_dim_sql_df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) Artist Dimension Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['artist_id', 'artist_latitude', 'artist_location', 'artist_longitude', 'artist_name', 'duration', 'num_songs', 'song_id', 'title', 'year']\n",
      "['artist', 'auth', 'firstName', 'gender', 'itemInSession', 'lastName', 'length', 'level', 'location', 'method', 'page', 'registration', 'sessionId', 'song', 'status', 'ts', 'userAgent', 'userId']\n"
     ]
    }
   ],
   "source": [
    "print(songs_df.columns)\n",
    "print(events_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore SPARK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs_df.select('itemInSession','ts', 'userId').describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = logs_df.withColumn(colName = 'MultiplyitemInSession',\n",
    "                   col = logs_df.itemInSession* 22)\n",
    "test_df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = test_df.withColumnRenamed(existing=\"page\", new =\"page_type\")\n",
    "test_df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = test_df.drop('itemInSession')\n",
    "test_df.show(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = test_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = test_df.filter(test_df.gender ==\"M\")\n",
    "test_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.groupBy('userId').sum('length').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.groupBy('userId').count().orderBy('count', ascending = False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@udf('string')\n",
    "def toUpper(word:str):\n",
    "    return word.upper()\n",
    "\n",
    "# upper_udf = udf(toUpper, returnType='string')\n",
    "test_df.select('userId',toUpper('firstName')).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@udf('string')\n",
    "def surrogateKey(text_values: List[str]):\n",
    "    sha256 = hashlib.sha256()\n",
    "    data = ''.join(text_values)\n",
    "    for value in text_values:\n",
    "        data = data + str(value)\n",
    "    sha256.update(data.encode())\n",
    "    return sha256.hexdigest()\n",
    "\n",
    "test_df.withColumn(colName = \"sur_key\",\n",
    "                   col = surrogateKey(array(test_df.userId, test_df.firstName, test_df.lastName)))\\\n",
    "        .select('sur_key')\\\n",
    "        .show(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = test_df.withColumn(colName=\"firstName\",\n",
    "                             col = toUpper(\"firstName\"))\n",
    "test_df.show(3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, explode, sequence, year, month, dayofweek, dayofmonth, dayofyear, weekofyear, date_format, lit\n",
    "from pyspark.sql.types import DateType\n",
    "\n",
    "@udf('int')\n",
    "def get_quarter_of_year(date):\n",
    "    return (date.month - 1) // 3 +1\n",
    "\n",
    "# Date range\n",
    "start_date = \"2003-01-01\"\n",
    "end_date = \"2005-12-31\"\n",
    "\n",
    "date_range_df = spark.sql(f\"SELECT explode(sequence(to_date('{start_date}'), to_date('{end_date}'), interval 1 day)) as date_day\")\n",
    "date_range_df.show(5)\n",
    "\n",
    "date_dim_df = date_range_df\\\n",
    "                .withColumn(colName=\"day_of_week\", col=dayofweek(\"date_day\")) \\\n",
    "                .withColumn(colName=\"day_of_month\", col=dayofmonth(\"date_day\")) \\\n",
    "                .withColumn(colName=\"day_of_year\", col=dayofyear(\"date_day\")) \\\n",
    "                .withColumn(colName=\"week_of_year\", col=weekofyear(\"date_day\")) \\\n",
    "                .withColumn(colName=\"month_of_year\", col=month(\"date_day\")) \\\n",
    "                .withColumn(colName=\"year_number\", col=year(\"date_day\")) \\\n",
    "                .withColumn(colName=\"month_name\", col=date_format(\"date_day\", \"MMMM\")) \\\n",
    "                .withColumn(colName=\"quarter_of_year\", col=get_quarter_of_year(\"date_day\"))\n",
    "date_dim_df.show(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_quarter_of_year(date):\n",
    "    return (date.month - 1) // 3 +1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quiz Questions\n",
    "\n",
    "## Question 1 :Which page did user id \"\" (empty string) NOT visit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_u_df = logs_df.filter(logs_df.userId ==\"\")\\\n",
    "                    .select('page')\\\n",
    "                    .alias(\"blank_pag\")\\\n",
    "                    .dropDuplicates()\n",
    "                    \n",
    "empty_u_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_visit_page = logs_df.select(\"page\").dropDuplicates()\n",
    "all_visit_page.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_page = set(all_visit_page.collect()) - set(empty_u_df.collect())\n",
    "print(diff_page)\n",
    "for row in diff_page:\n",
    "    print(row.page)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: What type of user does the empty string user id most likely refer to?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs_df.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_refer_df = logs_df.filter(logs_df.userId ==\"\")\\\n",
    "                        .select(\"level\")\\\n",
    "                        .alias(\"type_of_user\")\\\n",
    "                        .dropDuplicates()\n",
    "empty_refer_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3: How many female users do we have in the data set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "female_count = logs_df.filter(logs_df.gender =='F')\\\n",
    "                    .select('userId','gender')\\\n",
    "                    .dropDuplicates()\\\n",
    "                    .count() \n",
    "female_count   \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Question 4: How many songs were played from the most played artist?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs_df.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_songs_df = logs_df.filter(logs_df.page ==\"NextSong\")\\\n",
    "                       .select('artist')\\\n",
    "                       .groupby('artist')\\\n",
    "                       .agg(count(\"artist\").alias(\"Playcount\"))\\\n",
    "                       .sort(desc('Playcount'))\\\n",
    "                       .limit(10)\n",
    "most_songs_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5: How many songs do users listen to on average between visiting our home page? Please round your answer to the closest integer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def print_unique_values(df):\n",
    "    for column in df.columns:\n",
    "        unique_values = df.select(column).distinct().rdd.flatMap(lambda x: x).collect()\n",
    "        print(f\"{len(unique_values)} Unique values in column '{column}': {unique_values}\")\n",
    "        \n",
    "print_unique_values(logs_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs_df.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "songs_count_df = logs_df.filter(logs_df.page ==\"Home\")\\\n",
    "                        .select(\"userId\",\"song\",\"page\")\\\n",
    "                        .groupby(\"userId\")\\\n",
    "                        .agg(count(\"song\").alias(\"Total_Songs\"))\\\n",
    "                        .filter(\"Total_Songs > 0\")\n",
    "songs_count_df.show(5)                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_window = Window \\\n",
    "    .partitionBy('userID') \\\n",
    "    .orderBy(desc('ts')) \\\n",
    "    .rangeBetween(Window.unboundedPreceding, 0)\n",
    "    \n",
    "ishome = udf(lambda ishome : int(ishome == 'Home'), IntegerType())\n",
    "\n",
    "# Filter only NextSong and Home pages, add 1 for each time they visit Home\n",
    "# Adding a column called period which is a specific interval between Home visits\n",
    "cusum = logs_df.filter((logs_df.page == 'NextSong') | (logs_df.page == 'Home')) \\\n",
    "    .select('userID', 'page', 'ts') \\\n",
    "    .withColumn('homevisit', ishome(col('page'))) \\\n",
    "    .withColumn('period', Fsum('homevisit') \\\n",
    "    .over(user_window)) \n",
    "    \n",
    "# This will only show 'Home' in the first several rows due to default sorting\n",
    "\n",
    "cusum.show(5)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See how many songs were listened to on average during each period\n",
    "new_sum = cusum.filter((cusum.page == 'NextSong')) \\\n",
    "    .groupBy('userID', 'period') \\\n",
    "    .agg(count(\"period\").alias(\"total_count\")) \\\n",
    "    .agg(avg(\"total_count\").alias(\"average\")) \\\n",
    "    # .show()\n",
    "new_sum.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQL QUESTIONS\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs_df.createOrReplaceTempView(\"user_log_table\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "spark.sql('''\n",
    "          SELECT * \n",
    "          FROM user_log_table \n",
    "          LIMIT 2\n",
    "          '''\n",
    "          ).show()\n",
    "\n",
    "spark.sql('''\n",
    "          SELECT COUNT(*) \n",
    "          FROM user_log_table \n",
    "          '''\n",
    "          ).show()\n",
    "\n",
    "spark.sql('''\n",
    "          SELECT userID, firstname, page, song\n",
    "          FROM user_log_table \n",
    "          WHERE userID == '1046'\n",
    "          '''\n",
    "          ).show()\n",
    "\n",
    "spark.sql('''\n",
    "          SELECT DISTINCT page\n",
    "          FROM user_log_table \n",
    "          ORDER BY page ASC\n",
    "          '''\n",
    "          ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # User Defined Functions\n",
    "\n",
    "spark.udf.register(\"get_hour\", lambda x: int(datetime.datetime.fromtimestamp(x / 1000.0).hour))\n",
    "\n",
    "spark.sql('''\n",
    "          SELECT *, get_hour(ts) AS hour\n",
    "          FROM user_log_table \n",
    "          LIMIT 1\n",
    "          '''\n",
    "          ).show(5)\n",
    "\n",
    "songs_in_hour_df = spark.sql('''\n",
    "          SELECT get_hour(ts) AS hour, COUNT(*) as plays_per_hour\n",
    "          FROM user_log_table\n",
    "          WHERE page = \"NextSong\"\n",
    "          GROUP BY hour\n",
    "          ORDER BY cast(hour as int) ASC\n",
    "          '''\n",
    "          )\n",
    "\n",
    "songs_in_hour_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Question 1\n",
    "# \n",
    "# Which page did user id \"\" (empty string) NOT visit?\n",
    "\n",
    "# filter for users with blank user id\n",
    "blank_pages_query = \"\"\"\n",
    "    SELECT DISTINCT page AS blank_pages\n",
    "    FROM user_log_table\n",
    "    WHERE userId = ''\n",
    "\"\"\"\n",
    "\n",
    "# get a list of possible pages that could be visited\n",
    "all_pages_query = \"\"\"\n",
    "    SELECT DISTINCT page\n",
    "    FROM user_log_table\n",
    "\"\"\"\n",
    "\n",
    "# find values in all_pages that are not in blank_pages\n",
    "not_visited_pages_query = \"\"\"\n",
    "    SELECT all_pages.page\n",
    "    FROM ({all_pages_query}) all_pages\n",
    "    LEFT JOIN ({blank_pages_query}) blank_pages\n",
    "    ON all_pages.page = blank_pages.blank_pages\n",
    "    WHERE blank_pages.blank_pages IS NULL\n",
    "\"\"\".format(all_pages_query=all_pages_query, blank_pages_query=blank_pages_query)\n",
    "\n",
    "# Execute the queries\n",
    "blank_pages_df = spark.sql(blank_pages_query)\n",
    "all_pages_df = spark.sql(all_pages_query)\n",
    "not_visited_pages_df = spark.sql(not_visited_pages_query)\n",
    "not_visited_pages_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Question 3\n",
    "# \n",
    "# How many female users do we have in the data set?\n",
    "\n",
    "query = \"\"\"\n",
    "    SELECT COUNT(DISTINCT userId) AS count\n",
    "    FROM user_log_table\n",
    "    WHERE gender = 'F'\n",
    "\"\"\"\n",
    "\n",
    "# Execute the query\n",
    "result = spark.sql(query)\n",
    "\n",
    "# Print the result\n",
    "result.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Question 4\n",
    "# \n",
    "# How many songs were played from the most played artist?\n",
    "\n",
    "query = \"\"\"\n",
    "    SELECT Artist, COUNT(Artist) AS Playcount\n",
    "    FROM user_log_table\n",
    "    WHERE page = 'NextSong'\n",
    "    GROUP BY Artist\n",
    "    ORDER BY Playcount DESC\n",
    "    LIMIT 1\n",
    "\"\"\"\n",
    "\n",
    "# Execute the query\n",
    "result = spark.sql(query)\n",
    "\n",
    "# Print the result\n",
    "result.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "    WITH cusum AS (\n",
    "        SELECT userID, page, ts,\n",
    "            CASE WHEN page = 'Home' THEN 1 ELSE 0 END AS homevisit,\n",
    "            SUM(CASE WHEN page = 'Home' THEN 1 ELSE 0 END) OVER (\n",
    "                PARTITION BY userID\n",
    "                ORDER BY ts DESC\n",
    "                ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW\n",
    "            ) AS period\n",
    "        FROM user_log_table\n",
    "        WHERE page IN ('NextSong', 'Home')\n",
    "    )\n",
    "    SELECT AVG(song_count) AS average_songs\n",
    "    FROM (\n",
    "        SELECT userID, period, COUNT(period) AS song_count\n",
    "        FROM cusum\n",
    "        WHERE page = 'NextSong'\n",
    "        GROUP BY userID, period\n",
    "    )\n",
    "\"\"\"\n",
    "\n",
    "# Execute the query\n",
    "result = spark.sql(query)\n",
    "\n",
    "# Print the result\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aws_3_9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
