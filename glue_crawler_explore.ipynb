{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Data Catalog with Glue Crawler\n",
    "  WS Glue Crawler is a powerful automated tool offered by AWS Glue for discovering and cataloging metadata about data sources, which enables services like Glue and Athena to query data directly from different sources. By simply pointing the crawler to your data source, whether it's a database or a data lake, it will automatically scan and extract the schema information, data types, and other relevant metadata. This metadata is then organized and stored in a database table in the AWS Glue Data Catalog, providing a centralized repository for understanding and managing your data assets. In this lab, you will create a crawler that will be in charge of populating the Glue Data Catalog with the newly transformed data in S3, you will be able to query the data directly from S3 using Athena in the final part.\n",
    "\n",
    "Start by checking the databases in the data catalog using AWS SDK for pandas (`awswrangler`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "import gzip\n",
    "import json\n",
    "from typing import Dict\n",
    "\n",
    "import awswrangler as wr\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import smart_open\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "databases = wr.catalog.databases()\n",
    "print(databases)\n",
    "DATABASE_NAME = \"de-c3w2lab1-aws-reviews\"\n",
    "if DATABASE_NAME not in databases.values:\n",
    "    wr.catalog.create_database(DATABASE_NAME)\n",
    "    print(wr.catalog.databases())\n",
    "else:\n",
    "    print(f\"Database {DATABASE_NAME} already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glue_client = boto3.client('glue',region_name=\"us-east-1\")\n",
    "configuration= {\"Version\": 1.0,\"Grouping\": {\"TableGroupingPolicy\": \"CombineCompatibleSchemas\" }}\n",
    "CRAWLER_NAME = \"de-c3w2lab1-crawler\"\n",
    "response = glue_client.create_crawler(\n",
    "    Name= CRAWLER_NAME,\n",
    "    ### START CODE HERE ### (12 lines of code)\n",
    "    Role='Cloud9-de-c3w2lab1-glue-role', # @REPLACE EQUALS 'None',\n",
    "    DatabaseName=DATABASE_NAME, # @KEEP\n",
    "    Description='Amazon Reviews for toys', # @REPLACE EQUALS 'None',\n",
    "    Targets={ # @KEEP\n",
    "        'S3Targets': [ # @KEEP\n",
    "            { # @KEEP\n",
    "                'Path': f's3://{BUCKET_NAME}/processed_data/snappy/partition_by_year_month/toys_reviews/', # @REPLACE 'Path': f's3://{None}/processed_data/snappy/partition_by_year_month/toys_reviews/',\n",
    "            },# @KEEP\n",
    "            { # @KEEP\n",
    "                'Path': f's3://{BUCKET_NAME}/processed_data/snappy/partition_by_sales_category/toys_metadata/', # @REPLACE 'Path': f's3://{None}/processed_data/snappy/partition_by_sales_category/toys_metadata/',\n",
    "            } # @KEEP            \n",
    "        ]} # @KEEP\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = glue_client.list_crawlers()\n",
    "print(response['CrawlerNames'])\n",
    "response = glue_client.start_crawler(\n",
    "    Name= CRAWLER_NAME\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The crawler will start looking for data sources in the S3 targets that we have defined, it should take around 3 minutes for the first run.\n",
    "\n",
    "After around 3 minutes, check that the two tables were created for each processed dataset using the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wr.catalog.tables(database=DATABASE_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Data Querying with Athena"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.simplefilter('ignore', FutureWarning)\n",
    "sql = \"SELECT * FROM toys_reviews LIMIT 5\"\n",
    "df = wr.athena.read_sql_query(sql, database=DATABASE_NAME, s3_output=f's3://{BUCKET_NAME}/athena_output/')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Next, run this test query to find the top 5 products with the most reviews, return the name of the product and the count of reviews.\n",
    "# Ignore the products with an empty title in the metadata table.\n",
    "sql = \"\"\"\n",
    "SELECT met.title, count(distinct toy.reviewerid) as review_count\n",
    "FROM toys_metadata met \n",
    "LEFT JOIN toys_reviews toy\n",
    "ON met.asin = toy.asin\n",
    "WHERE met.title <> ''\n",
    "GROUP BY met.title\n",
    "ORDER BY count(distinct toy.reviewerid) DESC\n",
    "LIMIT 5\"\"\"\n",
    "df = wr.athena.read_sql_query(sql, database=DATABASE_NAME, s3_output=f's3://{BUCKET_NAME}/athena_output/')\n",
    "df.head()\n",
    "\n",
    "\n",
    "#Now run the next test query to find the top 10 products in terms of average rating, but the products should have at least 1000 reviews.\n",
    "# Return the title, sales category and the average rating.\n",
    "sql = \"\"\"\n",
    "SELECT met.title, met.sales_category, avg(toy.overall) as review_avg\n",
    "FROM toys_metadata met \n",
    "LEFT JOIN toys_reviews toy\n",
    "ON met.asin = toy.asin\n",
    "GROUP BY met.title, met.sales_category\n",
    "HAVING count(distinct toy.reviewerid) > 1000\n",
    "ORDER BY avg(toy.overall) DESC\n",
    "LIMIT 10\"\"\"\n",
    "df = wr.athena.read_sql_query(sql, database=DATABASE_NAME, s3_output=f's3://{BUCKET_NAME}/athena_output/')\n",
    "df.head()\n",
    "\n",
    "#Next, run this query to determine the average rating for each brand and the number of products they have in the database. \n",
    "# Only show the top 10 brands with the highest product counts in the database.\n",
    "sql = \"\"\"\n",
    "SELECT met.brand, count(distinct met.asin) as product_count, avg(toy.overall) as review_avg\n",
    "FROM toys_metadata met \n",
    "LEFT JOIN toys_reviews toy\n",
    "ON met.asin = toy.asin\n",
    "WHERE met.brand <> ''\n",
    "GROUP BY met.brand\n",
    "ORDER BY count(distinct toy.asin) DESC\n",
    "LIMIT 10\"\"\"\n",
    "df = wr.athena.read_sql_query(sql, database=DATABASE_NAME, s3_output=f's3://{BUCKET_NAME}/athena_output/')\n",
    "df.head()\n",
    "\n",
    "# For the last test query, you want to look at 25 random reviews that gave a rating of 5 to a toy car product. Return the product title, description, the review text and overall score. \n",
    "# Ignore products that have an empty title, use the LIKE operator to search reviews with toy car in their text.\n",
    "sql = \"\"\"\n",
    "SELECT met.title, met.description, toy.reviewtext, toy.overall\n",
    "FROM toys_reviews toy\n",
    "LEFT JOIN toys_metadata met\n",
    "ON toy.asin = met.asin\n",
    "WHERE toy.reviewtext like '%toy car%' and toy.overall = 5.0 and met.title <> '' \n",
    "LIMIT 25\"\"\"\n",
    "df = wr.athena.read_sql_query(sql, database=DATABASE_NAME, s3_output=f's3://{BUCKET_NAME}/athena_output/')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='7'></a>\n",
    "## COmparing Partitioning and Compression Features of Parquet Format\n",
    "\n",
    "**Compression**\n",
    "\n",
    "The Parquet format, commonly used in data lake architectures, supports various [compression codecs](https://parquet.apache.org/docs/file-format/data-pages/compression/) such as Snappy, Gzip, and LZO. These codecs play a crucial role in reducing the storage space needed by Parquet files. However, they come with a trade-off: while they reduce storage costs and optimize resource utilization, they can also impact processing speeds during data ingestion, transformation, and querying. To test this with a practical example, you will store the same transformed data using three different compression options: `UNCOMPRESSED`, `SNAPPY` and `GZIP`: \n",
    "\n",
    "- `UNCOMPRESSED`: Data is left uncompressed.\n",
    "- `SNAPPY`: Snappy is a fast compression/decompression library that works well with Parquet files. It provides good compression ratios and is optimized for speed. Snappy compression is often a good choice for balancing compression efficiency and query performance.\n",
    "- `GZIP`: Gzip is a widely used compression algorithm that provides high compression ratios. However, it tends to be slower in both compression and decompression compared to Snappy. Gzip compression can achieve higher levels of compression but may result in slower query performance.\n",
    "\n",
    "**Partitioning**\n",
    "\n",
    "Partitioning in Parquet is a technique used to better organize data within Parquet files based on partition keys. By partitioning data, Parquet optimizes query performance by reducing the amount of data that needs to be scanned during query execution. \n",
    "\n",
    "**Experiments**\n",
    "\n",
    "To explore the compression features, you will process the product metadata 3 times; in each time, you will choose a different option for the compression algorithm with no partitioning as shown in the left table here. Then you will compare for this data, \"uncompressed versus Snappy\" and then \"Snappy versus Gzip\". \n",
    "\n",
    "To explore the partitioning features, you will process the review dataset 3 times; in each time, you will choose a different option for the partitioning column, with Snappy for compression. Then you will compare for this data, \"partitioning versus no partitioning\" and then \"partitioning by year and month\" versus \"partitioning by the product id (asin)\".\n",
    "\n",
    "<img src=\"images/experiment.png\" width=\"700\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    response = glue_client.delete_crawler(\n",
    "        Name= CRAWLER_NAME\n",
    "    )\n",
    "    print(\"Crawler deleted successfully\")\n",
    "except glue_client.exceptions.EntityNotFoundException:\n",
    "    print(\"Crawler does not exist or has already been deleted\")\n",
    "\n",
    "try:\n",
    "    wr.catalog.delete_database(name=DATABASE_NAME)\n",
    "    print(f\"Database {DATABASE_NAME} deleted successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Error deleting database: {e}\")\n",
    "    \n",
    "response = glue_client.list_crawlers()\n",
    "print(\"Remaining crawlers:\", response['CrawlerNames'])\n",
    "\n",
    "databases = wr.catalog.databases()\n",
    "print(\"Remaining databases:\", databases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aws_3_9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
